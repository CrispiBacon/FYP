{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting pdf data (I am the spooker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               tweet                  date  \\\n",
      "0                                                     Mar 9, 2019, 4:16 PM   \n",
      "1                                                Why  Mar 9, 2019, 4:28 PM   \n",
      "2   Today in “Manosphere Grift or Terrorist Recru...  Mar 9, 2019, 4:43 PM   \n",
      "3   How can anyone see this as a good luck charm?...  Mar 9, 2019, 4:46 PM   \n",
      "4                                  Hamms is good tho  Mar 9, 2019, 5:44 PM   \n",
      "\n",
      "                                              source     RT  user_tag  \\\n",
      "0  (Source: https://twitter.com/iamthespookster/s...  False     False   \n",
      "1  (Source: https://twitter.com/iamthespookster/s...  False      True   \n",
      "2  (Source: https://twitter.com/iamthespookster/s...   True      True   \n",
      "3  (Source: https://twitter.com/iamthespookster/s...   True      True   \n",
      "4  (Source: https://twitter.com/iamthespookster/s...  False      True   \n",
      "\n",
      "            account  ext_link link  \n",
      "0                       False       \n",
      "1  @LetsJetTogether     False       \n",
      "2   @mollycrabapple     False       \n",
      "3  @JackieMorrisArt      True       \n",
      "4       @Lubchansky     False       \n",
      "3002\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# STEP 1 - Read in Tweets from PDF\n",
    "pdf_path = \"data\\Twitter-iamthespookster-2019-08-04 (1).pdf\"\n",
    "with pdfplumber.open(pdf_path) as pdf:\n",
    "    raw_data = \"\\n\".join([page.extract_text() for page in pdf.pages if page.extract_text()])\n",
    "\n",
    "# STEP 2 - Extract Tweets from Raw Data\n",
    "dat = raw_data.split(\"\\n\")\n",
    "dat = dat[4:]  # Remove first four lines (heading/whitespace)\n",
    "\n",
    "# Find and remove line breaks\n",
    "LBs = [i for i, line in enumerate(dat) if line == \"\"]\n",
    "dat = [line for i, line in enumerate(dat) if i not in LBs]\n",
    "\n",
    "# Find the Line Containing the Source of Each Tweet\n",
    "source_line = [i for i, line in enumerate(dat) if \"(Source:\" in line]\n",
    "\n",
    "# Extract Tweet, Date, Source from Content\n",
    "tweets, dates, sources = [], [], []\n",
    "start = 0\n",
    "for i in source_line:\n",
    "    tweet_in = dat[start:i+1]\n",
    "    tweets.append(\" \".join(tweet_in[:-2]))\n",
    "    dates.append(tweet_in[-2])\n",
    "    sources.append(tweet_in[-1])\n",
    "    start = i + 1\n",
    "\n",
    "data = pd.DataFrame({\"tweet\": tweets, \"date\": dates, \"source\": sources})\n",
    "\n",
    "# STEP 3 - Extract Data from Tweet\n",
    "url_pattern = re.compile(r\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\")\n",
    "\n",
    "data[\"RT\"] = data[\"tweet\"].str.startswith(\"RT \")\n",
    "data[\"tweet\"] = data[\"tweet\"].str.replace(r\"^RT \", \"\", regex=True)\n",
    "\n",
    "data[\"user_tag\"] = data[\"tweet\"].str.contains(\"@\")\n",
    "data[\"account\"] = data[\"tweet\"].apply(lambda x: \",\".join(re.findall(r\"@\\w+\", x)))\n",
    "data[\"tweet\"] = data[\"tweet\"].apply(lambda x: re.sub(r\"@\\w+|:\", \"\", x))\n",
    "\n",
    "data[\"ext_link\"] = data[\"tweet\"].str.contains(\"http\")\n",
    "data[\"link\"] = data[\"tweet\"].apply(lambda x: \",\".join(url_pattern.findall(x)))\n",
    "data[\"tweet\"] = data[\"tweet\"].apply(lambda x: url_pattern.sub(\"\", x))\n",
    "\n",
    "print(data.head())\n",
    "print(data.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting excel data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Posts\n",
      "0  Mommy said not to talk to strangers..but she's...\n",
      "1  1.I was immune to getting hurt/killed/infected...\n",
      "2  I mean terrorist attacks happen all the time. ...\n",
      "3  As a god, it would be my responsibility not to...\n",
      "4  I am going to grab a knife and shove it in the...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read CSV without headers and assign column names\n",
    "df2 = pd.read_excel('data\\Dataset.xlsx', names=['Posts'])\n",
    "\n",
    "print(df2.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine into 1 df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  Posts  Mass shooter post\n",
      "0                                                                        1\n",
      "1                                                   Why                  1\n",
      "2      Today in “Manosphere Grift or Terrorist Recru...                  1\n",
      "3      How can anyone see this as a good luck charm?...                  1\n",
      "4                                     Hamms is good tho                  1\n",
      "...                                                 ...                ...\n",
      "3851  I'm getting over much of the problems I've had...                  1\n",
      "3852  Bleys, thanks for your uplifting message.\\n\\nI...                  1\n",
      "3853  I'm a fan of zombie film's, have been for year...                  1\n",
      "3854  Hello All...\\n\\nI came across this place on th...                  1\n",
      "3855  Thank's for the friendly welcome, I'm going to...                  1\n",
      "\n",
      "[3856 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#combine the 2 data\n",
    "\n",
    "df1_selected = data[['tweet']].rename(columns={'tweet': 'Posts'})\n",
    "\n",
    "df = pd.concat([df1_selected,df2], ignore_index = True)\n",
    "df['Mass shooter post']=1\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
