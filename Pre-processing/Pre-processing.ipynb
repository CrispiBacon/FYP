{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df1 = pd.read_excel('Dataset_labelled.xlsx', header=0)\n",
    "df2 = pd.read_csv(r'tweets_labelled.csv', header=0)\n",
    "df3 = pd.read_csv(r'..\\data\\Threat Texts.csv', encoding='cp1252', header=None,  names=['Threat_Text'])\n",
    "df4 = pd.read_csv(r'..\\data\\synthetic_social_media_data.csv', header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3[\"Annotation\"] = 2 \n",
    "df4[\"Annotation\"] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering out incomplete post from dataset 2\n",
    "df2 = df2[~df2['tweet'].str.endswith('...', na=False)]\n",
    "\n",
    "# Filtering out data with external link from dataset 2\n",
    "df2 = df2[df2['ext_link'].fillna(False) == False]\n",
    "\n",
    "#Filtering out Negative data from dataset 4\n",
    "#df4 = df4[df4['Sentiment Label'] != \"Negative\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Column Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1[[\"Post\",\"Annotation\"]]\n",
    "df2 = df2[[\"tweet\",\"Annotation\"]]\n",
    "df3 = df3[[\"Threat_Text\", \"Annotation\"]]\n",
    "df4 = df4[[\"Post Content\",\"Annotation\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rename Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.rename(columns={\"Post\": \"text\"}, inplace=True)\n",
    "df2.rename(columns={\"tweet\": \"text\"}, inplace=True)\n",
    "df3.rename(columns={\"Threat_Text\": \"text\"}, inplace=True)\n",
    "df4.rename(columns={\"Post Content\": \"text\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5127 entries, 0 to 5126\n",
      "Data columns (total 2 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   text        5125 non-null   object \n",
      " 1   Annotation  5127 non-null   float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 80.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df_merged = pd.concat([df1, df2, df3, df4], ignore_index=True)\n",
    "\n",
    "df_merged.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Missing Value and Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 5099 entries, 0 to 5126\n",
      "Data columns (total 2 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   text        5099 non-null   object \n",
      " 1   Annotation  5099 non-null   float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 119.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df_merged = df_merged.dropna()\n",
    "df_merged = df_merged.drop_duplicates(subset=[\"text\"])\n",
    "\n",
    "df_merged.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'nltk' has no attribute 'internals' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01memoji\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcontractions\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m word_tokenize\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\nltk\\__init__.py:180\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cluster\n\u001b[1;32m--> 180\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdownloader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m download, download_shell\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtkinter\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\nltk\\downloader.py:2475\u001b[0m\n\u001b[0;32m   2465\u001b[0m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m   2468\u001b[0m \u001b[38;5;66;03m######################################################################\u001b[39;00m\n\u001b[0;32m   2469\u001b[0m \u001b[38;5;66;03m# Main:\u001b[39;00m\n\u001b[0;32m   2470\u001b[0m \u001b[38;5;66;03m######################################################################\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2473\u001b[0m \n\u001b[0;32m   2474\u001b[0m \u001b[38;5;66;03m# Aliases\u001b[39;00m\n\u001b[1;32m-> 2475\u001b[0m _downloader \u001b[38;5;241m=\u001b[39m \u001b[43mDownloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2476\u001b[0m download \u001b[38;5;241m=\u001b[39m _downloader\u001b[38;5;241m.\u001b[39mdownload\n\u001b[0;32m   2479\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdownload_shell\u001b[39m():\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\nltk\\downloader.py:515\u001b[0m, in \u001b[0;36mDownloader.__init__\u001b[1;34m(self, server_index_url, download_dir)\u001b[0m\n\u001b[0;32m    513\u001b[0m \u001b[38;5;66;03m# decide where we're going to save things to.\u001b[39;00m\n\u001b[0;32m    514\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_download_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 515\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_download_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault_download_dir\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\nltk\\downloader.py:1069\u001b[0m, in \u001b[0;36mDownloader.default_download_dir\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1066\u001b[0m \u001b[38;5;66;03m# Check if we have sufficient permissions to install in a\u001b[39;00m\n\u001b[0;32m   1067\u001b[0m \u001b[38;5;66;03m# variety of system-wide locations.\u001b[39;00m\n\u001b[0;32m   1068\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m nltkdir \u001b[38;5;129;01min\u001b[39;00m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mpath:\n\u001b[1;32m-> 1069\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(nltkdir) \u001b[38;5;129;01mand\u001b[39;00m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minternals\u001b[49m\u001b[38;5;241m.\u001b[39mis_writable(nltkdir):\n\u001b[0;32m   1070\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m nltkdir\n\u001b[0;32m   1072\u001b[0m \u001b[38;5;66;03m# On Windows, use %APPDATA%\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: partially initialized module 'nltk' has no attribute 'internals' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import emoji\n",
    "import nltk\n",
    "import contractions\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, words\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from autocorrect import Speller\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"words\")\n",
    "\n",
    "# Initialize tools\n",
    "spell = Speller(lang=\"en\")  # Autocorrect spelling\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "english_vocab = set(words.words())\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    # Text Normalization\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = contractions.fix(text)  # Expand contractions (\"can't\" → \"cannot\")\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()  # Remove excessive whitespace\n",
    "\n",
    "    # Remove unwanted cahracters\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()  # Remove HTML tags\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s#@]\", \"\", text)  # Keep alphanumeric, hashtags, mentions\n",
    "\n",
    "    # 3️Handle URLS, hashtags, mentions\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)  # Remove URLs\n",
    "    text = re.sub(r\"@\\w+\", \"USER\", text)  # Replace @mentions with \"USER\"\n",
    "    text = re.sub(r\"#(\\w+)\", lambda m: \" \".join(re.findall(r\"[A-Z]?[a-z]+|\\d+\", m.group(1))), text)  # Split hashtags\n",
    "\n",
    "    # Handle emoji\n",
    "    text = emoji.demojize(text, delimiters=(\" \", \" \"))  \n",
    "    text = re.sub(r\":\\S+:\", lambda m: m.group(0).replace(\"_\", \" \"), text)  \n",
    "\n",
    "    # Tokenization and remove stop word\n",
    "    words_list = word_tokenize(text)  # Tokenize text\n",
    "    words_list = [word for word in words_list if word not in stop_words]  # Remove stopwords\n",
    "\n",
    "    #autocorrect\n",
    "    words_list = [spell(word) if word not in english_vocab else word for word in words_list]\n",
    "\n",
    "    # Lemmatization\n",
    "    words_list = [lemmatizer.lemmatize(word) for word in words_list]\n",
    "\n",
    "    return \" \".join(words_list)  # Convert list back to string\n",
    "\n",
    "df_merged[\"cleaned_text\"] = df_merged[\"text\"].apply(preprocess_text)\n",
    "\n",
    "pd.set_option('display.max_rows', None)  # Show all rows\n",
    "print(df_merged)  # Display the full DataFrame\n",
    "pd.reset_option('display.max_rows')  # Reset to default after viewing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
